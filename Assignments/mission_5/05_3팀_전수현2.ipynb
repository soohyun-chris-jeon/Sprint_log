{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e31a0e9",
   "metadata": {},
   "source": [
    "# 스프린트 미션 5\n",
    "\n",
    "### 3팀 전수현\n",
    "\n",
    "### 제출일자: 2025.06.24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc42f7a",
   "metadata": {},
   "source": [
    "## 과제 목표: Autoencoder 기반 Denoising\n",
    "\n",
    "1. 최종 목표: 'denoising-dirty-documents' 데이터셋의 노이즈 낀 문서 이미지를 깨끗한 원본 이미지로 복원하는 딥러닝 모델을 만든다.\n",
    "2. 타겟 모델: Autoencoder\n",
    "3. 중점 과제:\n",
    "   - albumentations를 활용하여 Data Augmentation을 적극적으로 활용하여 모델의 일반화 성능을 높인다.\n",
    "   - W&B를 연동하여 모든 실험 과정을 추적하고 시각화하여 최적의 모델 성능을 찾는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ac265",
   "metadata": {},
   "source": [
    "이번 과제에서 중점 적으로 활용할 패키지는 `albumentations`와 `W&B`이다. 전통적인 DAE 문제를 푸는 과정을 최신 패키지 적용을 통해서 구현해보는 것이 개인적인 목표이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf0730",
   "metadata": {},
   "source": [
    "## 0. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bb8db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soohyun/miniconda3/envs/JSH/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm  # 진행 상황을 보여주는 바\n",
    "\n",
    "# PyTorch 라이브러리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 1. 데이터 증강(Augmentation) 라이브러리\n",
    "# torchvision도 좋지만, 이미지 변환에 더 강력하고 직관적인 albumentations를 사용\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 2. 실험 추적과 하이퍼 파라미터 튜닝을 위한 W&B 라이브러리 불러오기\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9999fa8",
   "metadata": {},
   "source": [
    "## 1. Set Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b28a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# STEP 1: 각종 Configuration 관리\n",
    "# ====================================================================\n",
    "# 나중에 하이퍼파라미터 튜닝을 편하게 하려면, 이렇게 설정값을 모아두는 습관이 중요함.\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 주요 하이퍼 파라미터\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "# data path 설정\n",
    "DATA_PATH = \"./data/denoising-dirty-documents/\"  # 데이터셋이 있는 경로\n",
    "train_dir = os.path.join(DATA_PATH, \"train/train\")\n",
    "train_cleaned_dir = os.path.join(DATA_PATH, \"train_cleaned/train_cleaned\")\n",
    "test_dir = os.path.join(DATA_PATH, \"test/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8d79ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 144\n",
      "Train Cleaned: 144\n",
      "Test: 72\n"
     ]
    }
   ],
   "source": [
    "# 해당 데이터셋의 이미지를\n",
    "def load_images_from_folder(train_dir):\n",
    "    images = []\n",
    "    for filename in os.listdir(train_dir):\n",
    "        if filename.endswith(\".png\"):  # PNG 파일만 가져오기\n",
    "            img_path = os.path.join(train_dir, filename)\n",
    "            img = cv2.imread(img_path)  # OpenCV로 이미지를 읽음 (BGR 형식)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR → RGB 변환\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images = load_images_from_folder(train_dir)\n",
    "train_cleaned_images = load_images_from_folder(train_cleaned_dir)\n",
    "test_images = load_images_from_folder(test_dir)\n",
    "\n",
    "\n",
    "print(f\"Train: {len(train_images)}\")\n",
    "print(f\"Train Cleaned: {len(train_cleaned_images)}\")\n",
    "print(f\"Test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330382",
   "metadata": {},
   "source": [
    "데이터셋의 갯수는 저렇게 구성되어 있는데 문제는 data의 크기가 일정하지 않다는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e086ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 사이즈는 몇종류일까 from baseline code\n",
      "Unique sizes in train:\n",
      " [[258 540]\n",
      " [420 540]]\n",
      "Unique sizes in train_cleaned:\n",
      " [[258 540]\n",
      " [420 540]]\n",
      "Unique sizes in test:\n",
      " [[258 540]\n",
      " [420 540]]\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터셋의 이미지 크기 추출\n",
    "train_sizes = [\n",
    "    img.shape[:2] for img in train_images\n",
    "]  # train 이미지의 크기 (height, width)\n",
    "train_cleaned_sizes = [\n",
    "    img.shape[:2] for img in train_cleaned_images\n",
    "]  # train_cleaned 이미지의 크기\n",
    "test_sizes = [img.shape[:2] for img in test_images]  # test 이미지의 크기\n",
    "\n",
    "\n",
    "train_unique_sizes = np.unique(train_sizes, axis=0)  # train 이미지의 유니크 크기\n",
    "train_cleaned_unique_sizes = np.unique(\n",
    "    train_cleaned_sizes, axis=0\n",
    ")  # train_cleaned 유니크 크기\n",
    "test_unique_sizes = np.unique(test_sizes, axis=0)  # test 유니크 크기\n",
    "# train_unique_sizes\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(\"데이터 사이즈는 몇종류일까 from baseline code\")\n",
    "print(\"Unique sizes in train:\\n\", train_unique_sizes)\n",
    "print(\"Unique sizes in train_cleaned:\\n\", train_cleaned_unique_sizes)\n",
    "print(\"Unique sizes in test:\\n\", test_unique_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46290fd",
   "metadata": {},
   "source": [
    "데이터 사이즈는 2가지 형태가 있기 때문에 추가적인 pre-processing 작업이 필요하다. 두 가지 경우의 수가 있는데,  \n",
    "1) resize를 하거나\n",
    "2) padding을 하거나  \n",
    "\n",
    "그러나 이번 과제는 denoising 과제이므로 resizing의 경우 문서의 비율이 깨져서 글자의 특성을 망가뜨릴 수 있기 때문에 padding이 더 적합한 프로세싱일 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b08cb",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "\n",
    "PyTorch의 Dataset 클래스를 상속받아서 우리 데이터셋에 맞는 클래스를 들고 데이터 로드까지.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 커스텀 데이터셋 클래스 구현\n",
    "# ====================================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, dirty_dir, clean_dir, transform=None):\n",
    "        self.dirty_dir = dirty_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.dirty_images = os.listdir(self.dirty_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dirty_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.dirty_images[index]\n",
    "        dirty_img_path = os.path.join(self.dirty_dir, img_name)\n",
    "        clean_img_path = os.path.join(\n",
    "            self.clean_dir, img_name\n",
    "        )  # clean 이미지 파일 이름이 같다고 가정\n",
    "\n",
    "        dirty_image = np.array(Image.open(dirty_img_path).convert(\"RGB\"))\n",
    "        clean_image = np.array(Image.open(clean_img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Augmentation 적용\n",
    "        if self.transform:\n",
    "            # DAE에서는 보통 입력(dirty)에만 augmentation을 적용해.\n",
    "            # 하지만 geometric 변환(회전, 뒤집기 등)은 clean 이미지에도 동일하게 적용해야 할 수도 있어.\n",
    "            # 여기서는 간단하게 dirty 이미지에만 적용하는 것으로 시작해보자.\n",
    "            augmented = self.transform(image=dirty_image)\n",
    "            dirty_image = augmented[\"image\"]\n",
    "        # TODO: dirty_image와 clean_image를 텐서로 변환해서 반환\n",
    "        return dirty_image, clean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94a18fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 540)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = TrainDataset(train_dir, train_cleaned_dir)\n",
    "for x, y in iter(train_data):\n",
    "    pass\n",
    "a = x.shape\n",
    "a[:2]\n",
    "# cv2.imshow(x)\n",
    "# np.unique(train_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ac0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 데이터 증강 (Data Augmentation)\n",
    "# ====================================================================\n",
    "# TODO: Albumentations를 사용해서 train과 validation용 변환 파이프라인 정의하기\n",
    "TARGET_HEIGHT = 540\n",
    "TARGET_WIDTH = 420\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.ToGray(p=1.0),  # RGB → Gray\n",
    "        A.LongestMaxSize(max_size=TARGET_WIDTH, interpolation=cv2.INTER_AREA),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=TARGET_HEIGHT,\n",
    "            min_width=TARGET_WIDTH,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "        ),  # zero-padding\n",
    "        # A.Resize(),\n",
    "        A.Rotate(limit=10, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # 정규화 및 텐서 변환\n",
    "        A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        # A.Resize(),\n",
    "        A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a49b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 데이터로더 생성\n",
    "# ====================================================================\n",
    "# TODO: DenoiseDataset과 DataLoader를 사용해서 train_loader, val_loader 만들기\n",
    "train_dataset = DenoiseDataset(...)\n",
    "train_loader = DataLoader(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf874d",
   "metadata": {},
   "source": [
    "## 3. Model implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928404c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# STEP 4: U-Net 모델\n",
    "# ====================================================================\n",
    "# U-Net을 구성하는 기본 블록 (Convolution 2번)\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        # TODO: Conv2d, BatchNorm2d, ReLU를 사용해서 DoubleConv 블록 구현\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  # x\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512]):\n",
    "        super(UNET, self).__init__()\n",
    "        # TODO: DoubleConv와 MaxPool2d를 사용해서 Encoder(Down) 부분 구현\n",
    "        # self.downs = nn.ModuleList()\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # TODO: DoubleConv와 ConvTranspose2d를 사용해서 Decoder(Up) 부분 구현\n",
    "        # self.ups = nn.ModuleList()\n",
    "\n",
    "        # TODO: 가장 아래쪽 Bottleneck 부분 구현\n",
    "        # self.bottleneck = DoubleConv(...)\n",
    "\n",
    "        # TODO: 최종 출력 레이어 (1x1 Conv) 구현\n",
    "        # self.final_conv = nn.Conv2d(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Encoder -> Bottleneck -> Decoder 순서로 forward pass 구현\n",
    "        # 중요한 건! Encoder에서 나온 출력을 skip_connections 리스트에 저장해뒀다가\n",
    "        # Decoder에서 하나씩 꺼내서 합쳐주는(concat) 거야.\n",
    "        # skip_connections = []\n",
    "        # for down in self.downs:\n",
    "        #     x = down(x)\n",
    "        #     skip_connections.append(x)\n",
    "        #     x = self.pool(x)\n",
    "        # ...\n",
    "        return  # x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976c69f",
   "metadata": {},
   "source": [
    "## 4. Train a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68419ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# STEP 5: 학습 루프 (Training Loop)\n",
    "# ====================================================================\n",
    "\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    # TODO: 1 에포크 동안의 학습을 진행하는 함수 구현\n",
    "    # model.train()\n",
    "    # loop = tqdm(loader)\n",
    "    # for batch_idx, (data, targets) in enumerate(loop):\n",
    "    #     data = data.to(device=DEVICE)\n",
    "    #     targets = targets.to(device=DEVICE)\n",
    "\n",
    "    #     # Forward\n",
    "    #     predictions = model(data)\n",
    "    #     loss = loss_fn(predictions, targets)\n",
    "\n",
    "    #     # Backward\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     # TODO: W&B에 loss 기록 (wandb.log)\n",
    "    #     loop.set_postfix(loss=loss.item())\n",
    "    pass\n",
    "\n",
    "\n",
    "def validate_fn(loader, model, loss_fn, device):\n",
    "    # TODO: validation 데이터에 대한 성능을 평가하고, 결과 이미지 몇 개를 시각화하는 함수 구현\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    # ...\n",
    "    # TODO: W&B에 validation loss와 결과 이미지 기록 (wandb.log)\n",
    "    # model.train()\n",
    "    pass\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# 메인 실행부\n",
    "# ====================================================================\n",
    "# TODO: 모델, Loss 함수, Optimizer 정의하기\n",
    "# model = UNET(in_channels=3, out_channels=3).to(DEVICE)\n",
    "# loss_fn = nn.MSELoss() # 또는 nn.L1Loss()가 이미지 복원에 더 좋다고도 해\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TODO: W&B 초기화 (wandb.init)\n",
    "# wandb.init(project=\"denoising-autoencoder\", config={...})\n",
    "\n",
    "\n",
    "# 학습 시작!\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    # train_fn(...)\n",
    "    # validate_fn(...)\n",
    "\n",
    "    # TODO: 매 에포크마다 또는 validation 성능이 가장 좋을 때 모델 저장하기\n",
    "    # torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6476f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# STEP 6: 평가 및 시각화\n",
    "# ====================================================================\n",
    "# TODO: 저장된 best_model.pth 불러오기\n",
    "# model.load_state_dict(...)\n",
    "\n",
    "# TODO: 테스트 데이터셋에서 몇 개의 이미지를 가져와서\n",
    "# [노이즈 이미지] vs [모델이 복원한 이미지] vs [원본 깨끗한 이미지]\n",
    "# 이렇게 3개를 나란히 놓고 비교하는 시각화 코드 작성 (matplotlib 사용)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

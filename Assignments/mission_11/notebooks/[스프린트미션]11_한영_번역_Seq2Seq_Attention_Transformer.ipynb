{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkmSQwxP_S1k",
        "outputId": "cda1cb15-7cc2-42fe-a6b8-402cef97ff8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y mecab mecab-ipadic-utf8 libmecab-dev\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "lg9Ub5h9AhZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "from konlpy.tag import Okt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# JSON 파일 경로\n",
        "train_json_file_path = \"/content/drive/Shareddrives/스프린트(AI) 드라이브/트랙 Master 폴더/스프린트 미션 및 모범답안/data/translation/일상생활및구어체_한영_train_set.json\"\n",
        "valid_json_file_path = \"/content/drive/Shareddrives/스프린트(AI) 드라이브/트랙 Master 폴더/스프린트 미션 및 모범답안/data/translation/일상생활및구어체_한영_valid_set.json\"\n",
        "\n",
        "# JSON 파일 불러오기\n",
        "def load_json(file_path, max_samples=1000):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data[\"data\"][:max_samples]\n",
        "\n",
        "# 훈련 및 검증 데이터 로드\n",
        "data_train = load_json(train_json_file_path, max_samples=50000)\n",
        "data_valid = load_json(valid_json_file_path, max_samples=1000)\n",
        "\n",
        "# ko와 mt 데이터 추출\n",
        "ko_sentences_train = [item[\"ko\"] for item in data_train]\n",
        "mt_sentences_train = [item[\"mt\"] for item in data_train]\n",
        "ko_sentences_valid = [item[\"ko\"] for item in data_valid]\n",
        "mt_sentences_valid = [item[\"mt\"] for item in data_valid]\n",
        "\n",
        "# 한국어 및 영어 토크나이저\n",
        "tokenizer_ko = Okt().morphs\n",
        "tokenizer_en = word_tokenize\n",
        "\n",
        "## 문장 길이 분석\n",
        "ko_lengths = [len(tokenizer_ko(sent)) for sent in ko_sentences_train]\n",
        "en_lengths = [len(tokenizer_en(sent)) for sent in mt_sentences_train]\n",
        "all_lengths = ko_lengths + en_lengths\n",
        "\n",
        "# 한국어와 영어 중 가장 긴 문장의 길이 기준으로 MAX_LENGTH 설정\n",
        "MAX_LENGTH = max(max(ko_lengths), max(en_lengths)) + 1  # SOS, EOS 포함 고려\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
        "\n",
        "# 특수 토큰 정의\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        # 초기에는 PAD, SOS, EOS, UNK 토큰을 미리 등록\n",
        "        self.word2index = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<unk>\"}\n",
        "        self.word2count = {}\n",
        "        self.n_words = 4  # PAD, SOS, EOS, UNK 포함\n",
        "\n",
        "    def addSentence(self, sentence, tokenizer):\n",
        "        for word in tokenizer(sentence):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.word2count[word] = 1\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# 데이터 준비\n",
        "def prepareData(lang1, lang2, tokenizer1, tokenizer2):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    pairs = list(zip(ko_sentences_train, mt_sentences_train))\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0], tokenizer1)\n",
        "        output_lang.addSentence(pair[1], tokenizer2)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(\"ko\", \"en\", tokenizer_ko, tokenizer_en)\n",
        "\n",
        "# 텐서 변환 및 데이터 로더 생성\n",
        "def tensorFromSentence(lang, sentence, tokenizer):\n",
        "    indexes = [SOS_token]\n",
        "    indexes += [lang.word2index.get(word, UNK_token) for word in tokenizer(sentence)[:MAX_LENGTH - 2]]\n",
        "    indexes.append(EOS_token)\n",
        "    # 길이 MAX_LENGTH에 맞춰 PAD 추가\n",
        "    while len(indexes) < MAX_LENGTH:\n",
        "        indexes.append(PAD_token)\n",
        "    return torch.tensor(indexes[:MAX_LENGTH], dtype=torch.long, device=device)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_tensors = [tensorFromSentence(input_lang, inp, tokenizer_ko) for inp, _ in pairs]\n",
        "    target_tensors = [tensorFromSentence(output_lang, tgt, tokenizer_en) for _, tgt in pairs]\n",
        "\n",
        "    input_tensors = torch.stack(input_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
        "    target_tensors = torch.stack(target_tensors, dim=0)  # [num_samples, MAX_LENGTH]\n",
        "\n",
        "    dataset = TensorDataset(input_tensors, target_tensors)\n",
        "    train_sampler = RandomSampler(dataset)\n",
        "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    print(f\"input_tensors.shape: {input_tensors.shape}, target_tensors.shape: {target_tensors.shape}\")\n",
        "    return train_dataloader\n",
        "\n",
        "train_dataloader = get_dataloader(batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejh_G_uL_Ypb",
        "outputId": "1594b90e-b9ff-4e3b-fc88-c4b0c2e2c91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 96\n",
            "Read 50000 sentence pairs\n",
            "input_tensors.shape: torch.Size([50000, 96]), target_tensors.shape: torch.Size([50000, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 문장\n",
        "print(ko_sentences_train[0])\n",
        "print(mt_sentences_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dumd_PBgqDbF",
        "outputId": "ee9f4f2e-877d-4500-98f9-96bf4aade8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n",
            "If you reply to the color you want, we will start making it right away.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증 데이터를 위한 pair 생성\n",
        "pairs_valid = list(zip(ko_sentences_valid, mt_sentences_valid))\n",
        "print(\"Read %s validation sentence pairs\" % len(pairs_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxlgOMcsO7lW",
        "outputId": "c22d2965-0ac8-48af-c2a8-31b573683c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 1000 validation sentence pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq"
      ],
      "metadata": {
        "id": "YjZsHdHjHVYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Encoder 정의\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "# Decoder 정의\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        # 초기 디코더 입력: SOS 토큰 ([batch_size, 1])\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=encoder_outputs.device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: target_tensor의 i번째 토큰 사용 ([batch_size, 1])\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            else:\n",
        "                # 모델 예측: topk 결과에서 가장 높은 확률을 가진 토큰 선택\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                # topi의 shape: [batch_size, 1, 1] → squeeze 마지막 차원만 제거하여 [batch_size, 1]\n",
        "                decoder_input = topi.squeeze(2).detach()\n",
        "\n",
        "        # [batch_size, MAX_LENGTH, output_size]\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        # input: [batch_size, 1]\n",
        "        output = self.embedding(input)          # → [batch_size, 1, hidden_size]\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden) # output: [batch_size, 1, hidden_size]\n",
        "        output = self.out(output)                 # → [batch_size, 1, output_size]\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "EL-2N9ykDLKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "                decoder_optimizer, criterion):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # 반드시 정수(LongTensor)로 변환\n",
        "        input_tensor = input_tensor.long().to(device)\n",
        "        target_tensor = target_tensor.long().to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def train_seq2seq(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100):\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch}/{n_epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "ZILQ_rS0NNYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화 및 학습 실행\n",
        "hidden_size = 128\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train_seq2seq(train_dataloader, encoder, decoder, n_epochs=30, print_every=1)"
      ],
      "metadata": {
        "id": "dTpkaGqtj1Hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac99a73-08db-4749-f995-581c39efa0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.7674\n",
            "Epoch 2/30, Loss: 0.5754\n",
            "Epoch 3/30, Loss: 0.5288\n",
            "Epoch 4/30, Loss: 0.4964\n",
            "Epoch 5/30, Loss: 0.4710\n",
            "Epoch 6/30, Loss: 0.4496\n",
            "Epoch 7/30, Loss: 0.4309\n",
            "Epoch 8/30, Loss: 0.4146\n",
            "Epoch 9/30, Loss: 0.4003\n",
            "Epoch 10/30, Loss: 0.3873\n",
            "Epoch 11/30, Loss: 0.3759\n",
            "Epoch 12/30, Loss: 0.3654\n",
            "Epoch 13/30, Loss: 0.3559\n",
            "Epoch 14/30, Loss: 0.3474\n",
            "Epoch 15/30, Loss: 0.3394\n",
            "Epoch 16/30, Loss: 0.3322\n",
            "Epoch 17/30, Loss: 0.3256\n",
            "Epoch 18/30, Loss: 0.3193\n",
            "Epoch 19/30, Loss: 0.3136\n",
            "Epoch 20/30, Loss: 0.3082\n",
            "Epoch 21/30, Loss: 0.3031\n",
            "Epoch 22/30, Loss: 0.2984\n",
            "Epoch 23/30, Loss: 0.2939\n",
            "Epoch 24/30, Loss: 0.2898\n",
            "Epoch 25/30, Loss: 0.2858\n",
            "Epoch 26/30, Loss: 0.2822\n",
            "Epoch 27/30, Loss: 0.2787\n",
            "Epoch 28/30, Loss: 0.2755\n",
            "Epoch 29/30, Loss: 0.2724\n",
            "Epoch 30/30, Loss: 0.2691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 단일 문장이므로 배치 차원 추가 (shape: [1, MAX_LENGTH])\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence, tokenizer_ko).unsqueeze(0)\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "\n",
        "        # <SOS>, <EOS>, SOS, EOS 등을 제거\n",
        "        tokens_to_remove = ['<SOS>', 'SOS', '<EOS>', 'EOS']\n",
        "        output_words = [w for w in output_words if w not in tokens_to_remove]\n",
        "\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "SyqePAMOTLAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "Q-Kyxxweny2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a42296-d4ec-4ffc-e4a2-1d8a95d2f05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 이 메일이 귀하께 잘 도착했기를 바랍니다.\n",
            "= I hope this mail has arrived well for you.\n",
            "< I hope you will receive the product as soon as possible .\n",
            "\n",
            "> 이 건 만큼은 제가 계속 체크하여 직접 연락드리도록 하겠습니다.\n",
            "= I'll keep checking this and contact you directly.\n",
            "< We will contact you again after arranging a few days .\n",
            "\n",
            "> 제가 한번 오피스텔을 보러 가도 될까요?\n",
            "= Can I go see the officetel?\n",
            "< Can I get a chance to see ?\n",
            "\n",
            "> 저희는 오랜 연구 끝에 피토알렉신을 함유한 콩 추출물이 피부 보호 능력에 뛰어남을 발견했습니다.\n",
            "= After a long study, we found that soybean extracts containing phyto-Alexin are excellent in skin protection.\n",
            "< It contains adenosine and niacinamide to brighten the dark and soft texture to the taste of the cold .\n",
            "\n",
            "> 커버는 탈부착과 세척이 가능하여 청소하기 쉽습니다.\n",
            "= The cover is removable and washable, making it easy to clean.\n",
            "< It is easy to carry in the form of three-piece water .\n",
            "\n",
            "> 그리고 우유를 조금 넣어야지 안 그러면 퍽퍽해서 살리지가 않아서 한 이 정도만 넣고 설탕 두 숟갈 정도.\n",
            "= And if you don't add a little milk, it's dry and doesn't save it, so add about this much and about two spoons of sugar.\n",
            "< And if you have a small amount , it is a little more firmly and more than a large amount of money .\n",
            "\n",
            "> 저희는 주문하면 최소 매주 10개 이상을 주문하려고 합니다.\n",
            "= When ordering, we try to order at least 10 pieces per week.\n",
            "< We will send you the address of the bank swift requested .\n",
            "\n",
            "> 급한 일이 있으시면, 제 핸드폰으로 연락을 주시면 됩니다.\n",
            "= If you have an emergency, you can contact me on my cell phone.\n",
            "< If you have any questions , please contact me .\n",
            "\n",
            "> 혹시 그 부분에 대해 알아봐 주실 수 있을까요?\n",
            "= Can you please find out about that?\n",
            "< Can you please let me know if you 're curious about ?\n",
            "\n",
            "> >왜 못 봐.\n",
            "= >Why can't I see it?\n",
            "< > Why did you see ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq with Attention"
      ],
      "metadata": {
        "id": "V_J9ShNCHY5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "RBQRVMNzEFUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화 및 학습 실행\n",
        "hidden_size = 128\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# 모델 학습 실행\n",
        "train_seq2seq(train_dataloader, encoder, decoder, n_epochs=30, print_every=1)"
      ],
      "metadata": {
        "id": "iP3faHSLH4Cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cad1f1-f3dc-43c8-9376-55979b8a42ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.7510\n",
            "Epoch 2/30, Loss: 0.5734\n",
            "Epoch 3/30, Loss: 0.5260\n",
            "Epoch 4/30, Loss: 0.4903\n",
            "Epoch 5/30, Loss: 0.4612\n",
            "Epoch 6/30, Loss: 0.4367\n",
            "Epoch 7/30, Loss: 0.4154\n",
            "Epoch 8/30, Loss: 0.3966\n",
            "Epoch 9/30, Loss: 0.3800\n",
            "Epoch 10/30, Loss: 0.3649\n",
            "Epoch 11/30, Loss: 0.3511\n",
            "Epoch 12/30, Loss: 0.3384\n",
            "Epoch 13/30, Loss: 0.3269\n",
            "Epoch 14/30, Loss: 0.3164\n",
            "Epoch 15/30, Loss: 0.3064\n",
            "Epoch 16/30, Loss: 0.2972\n",
            "Epoch 17/30, Loss: 0.2885\n",
            "Epoch 18/30, Loss: 0.2807\n",
            "Epoch 19/30, Loss: 0.2730\n",
            "Epoch 20/30, Loss: 0.2659\n",
            "Epoch 21/30, Loss: 0.2593\n",
            "Epoch 22/30, Loss: 0.2529\n",
            "Epoch 23/30, Loss: 0.2470\n",
            "Epoch 24/30, Loss: 0.2413\n",
            "Epoch 25/30, Loss: 0.2360\n",
            "Epoch 26/30, Loss: 0.2307\n",
            "Epoch 27/30, Loss: 0.2261\n",
            "Epoch 28/30, Loss: 0.2212\n",
            "Epoch 29/30, Loss: 0.2168\n",
            "Epoch 30/30, Loss: 0.2127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "CIMpZtUrn1Fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd44d2a1-fc9d-4fb6-d23e-ffc6b0b724be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> >이건 줘.\n",
            "= Give me this.\n",
            "< > You 're doing this .\n",
            "\n",
            "> 좋아요, 그리고 만약 미국 밖에서 생산된 차량이 요구되는 기준을 충족하지 못하면 어떻게 될까요?\n",
            "= Okay, and what if a vehicle manufactured outside the United States doesn't meet the required standards?\n",
            "< OK , if you have any other challenges , you can see that the vehicle is possible , but can you buy a standards ?\n",
            "\n",
            "> 알고 계실 수도 있지만, 저희 회사가 쇠퇴기에 접어들면서 많은 문제점들에 당면했습니다.\n",
            "= As you may be aware, as our company entered a period of decline, we faced many challenges.\n",
            "< As you may know , but we faced many problems with our company .\n",
            "\n",
            "> 정확한 레플리카를 원하시면 사진을 보내주세요.\n",
            "= If you are looking for an exact replica, please send us pictures as well.\n",
            "< If you want , please read carefully and photos .\n",
            "\n",
            "> 내가 말을 많이 하면 안 돼.\n",
            "= I can't talk too much.\n",
            "< I ca n't do it .\n",
            "\n",
            "> >나오나.\n",
            "= It's coming out.\n",
            "< There 's a left .\n",
            "\n",
            "> 이 상품은 온라인으로 가입 가능하신 상품으로 저희 은행 인터넷 뱅킹으로 신청하시면 됩니다.\n",
            "= This product is available online and you can apply through our bank's Internet banking.\n",
            "< This product can be compatible with the online and exchange free of charge .\n",
            "\n",
            "> 그리고 원하는 지역에 광고 배너 게시 프로세스를 지원하는 부서가 있습니다.\n",
            "= And we have a department assisting the ad banner posting process in the regions you would like.\n",
            "< And the company provides an advertisement platform in the website .\n",
            "\n",
            "> 1인 엔진 성능 진단, 진공 브레이크, 클러치 블리딩, 오일 이송, 배출, 샘플링, 윈드실드 수리, 터보차저 테스트 및 디젤 연료 프라이밍을 수행합니다.\n",
            "= One person performs engine performance diagnostics, vacuum brakes, clutch bleeding, oil transfers, drains, sampling, windshield repairs, turbocharger tests and diesel fuel priming.\n",
            "< It is a high-performance engine , model , and smart industries , carbon monoxide , mascara , fat , and cooling impact assessment .\n",
            "\n",
            "> >이 팀은 안 돼.\n",
            "= > This team can't\n",
            "< > This team is n't .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkdZ1rdkcm0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dP6Z4FLlciSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_U-AHTz4ShGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}